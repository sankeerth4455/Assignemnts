{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncQfYBezrQYN"
   },
   "source": [
    "# Lab 6: Recurrent neural networks\n",
    "\n",
    "In this lab you will use a recurrent neural network to predict whether or not a *tweet* is talking about a real disaster or not. To do this, we will use *Kaggle.com*'s competition [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started). Please follow the competition directions to obtain the data and evaluate your final model, noting the extra requirements below. **There is no requirement to actually submit your resutls to the competition.**\n",
    "\n",
    "**Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.**\n",
    "\n",
    "**Requirements**\n",
    " - Keras's `TextVectorization` functionality must be used, although it need not be part of the model\n",
    " - `train.csv` should be split into training and validation sets \n",
    " - the heart of your model must only use recurrent layers chosen from those [available in Keras](https://keras.io/api/layers/#recurrent-layers)\n",
    " - an embedding layer should be used; this can be learned along with the main task or use the [GloVe](https://github.com/stanfordnlp/GloVe) or [word2vec]() pretrained word embeddings\n",
    " - the evaluation metric for this dataset is the [F1-Score](https://www.kaggle.com/c/nlp-getting-started/overview/evaluation)\n",
    "\n",
    "**Grading:** \n",
    "\n",
    " - 50% of the grade will come from FINAL, error-free code written in Python/Keras that accomplishes all the steps outlined  \n",
    " - 50% will come from descriptive comments associated with that code, where the comments explain what the code is doing and why it is important to the overall objective; see example below\n",
    " \n",
    "```\n",
    "def one_hot_encode_token(token):\n",
    "    \"\"\"This function can be used to convert integer encoded vectors to one-hot-encoded vectors.\n",
    "    It processes one integer at a time and requires that vocabulary indexing already be done. \n",
    "    input: \n",
    "        token: an integer, e.g., 3\n",
    "    return:\n",
    "        vector: a one-hot vector of vocabulary length, [0, 0, 0, 1, 0,...]\n",
    "    \"\"\"\n",
    "    vector = np.zeros((len(vocabulary),))\n",
    "    vector[token] = 1\n",
    "    return vector\n",
    "```\n",
    "\n",
    "\n",
    "**What to submit:**\n",
    "- a copy of this notebook with:\n",
    "    - final, well-commented, error-free code in Python/Keras\n",
    "    - all code cells executed and output visible\n",
    "- a `submission.csv` file containing the predictions of your final model on the `test.csv` data\n",
    "- the final version of your model saved as a `Group_#_Lab_6.keras` file\n",
    "\n",
    "**What NOT to submit:**\n",
    " - data files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXeoBeH-VWww"
   },
   "source": [
    "## Group 10\n",
    "\n",
    "\n",
    "Keshav Yadav - 0770087\n",
    "\n",
    "Sri Sankeerth Koduru - 0768993\n",
    "\n",
    "Dilpreet Singh - 0771612\n",
    "\n",
    "Siva Sai Chaitanya Varma Sykam - 0770796"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "FSKZ2spcrQYU"
   },
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ngrams\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, LSTM, Dropout\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8G2-sBKKrwZG",
    "outputId": "4c8b963f-3b94-4821-f46c-21b662d3f9e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword  ...                                               text target\n",
       "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
       "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
       "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
       "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
       "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the dataset and looking at the top 5 values of the dataset\n",
    "data = pd.read_csv('train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RxYPMm3ENtG"
   },
   "source": [
    "#### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "DJrCDzOWx_DN"
   },
   "outputs": [],
   "source": [
    "\"\"\" Cleaning th data by dropping the id, keyword, location from the dataframe\n",
    "    input: \n",
    "        The entire training dataset\n",
    "    return:\n",
    "        Dataset with only the text and target files \"\"\"\n",
    "data = data.drop(['id','keyword','location'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Azw3aXqU3k_6"
   },
   "outputs": [],
   "source": [
    "\"\"\" Creating the vectorize class with a standardize finction. The standerdize function helps remove all the puctuations\n",
    "    as well as change all the letter to lower letters. This is needed to standardize the text and only keep the text.\n",
    "    input: \n",
    "        string: All residents asked to 'shelter in place' are\n",
    "    return:\n",
    "        string: all residents asked to shelter in place are \"\"\"\n",
    "class Vectorizer:\n",
    "    def standardize(self, input_text):\n",
    "        text = input_text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "v1 = Vectorizer()\n",
    "data['text'] = data['text'].apply(lambda x : v1.standardize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9JlcFhg1yL3a",
    "outputId": "b57178a9-ea58-4943-cce4-48e322157867"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  our deeds are the reason of this earthquake ma...       1\n",
       "1              forest fire near la ronge sask canada       1\n",
       "2  all residents asked to shelter in place are be...       1\n",
       "3  13000 people receive wildfires evacuation orde...       1\n",
       "4  just got sent this photo from ruby alaska as s...       1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the top 5 rows of the clean data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifVpLRrBEGSj"
   },
   "source": [
    "#### Vectorizong the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "cdCDdUk_pgLB"
   },
   "outputs": [],
   "source": [
    "\"\"\" converting the dataset into a dataframe so that the vectorization can be performed on the text data.\n",
    "    input: \n",
    "        dataframe:data['text']\n",
    "    return:\n",
    "        dataset: text \"\"\"\n",
    "text = tf.data.Dataset.from_tensor_slices((data['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlZn7BLU33Dw",
    "outputId": "80112d07-053c-4fdf-e98b-519dee97d152"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" The following steps are don in this code chunk:\n",
    "    The vectorization model was created using the TextVectorization function.This is important as this will be used to create the dictionary as well as the tokenizing of the text.\n",
    "    The .adapt fuction was used on the text to create a word dictionary which will be used to create the tokens on the text.\n",
    "    vectorize_layer was used on the data['text'] to convert the text to numeric data.\n",
    "    Padding the text so that none of the information is lost while running it in the model.\n",
    "    identifying the \n",
    "    input: \n",
    "        string: i am good\n",
    "    return:\n",
    "        numpy array:[15,27,1302]  \"\"\"\n",
    "vectorize_layer = TextVectorization(output_mode='int',max_tokens=20000, standardize='lower_and_strip_punctuation',split='whitespace')\n",
    "vectorize_layer.adapt(text)\n",
    "vectorized_text = vectorize_layer(data['text'])\n",
    "Final_Text = tf.keras.preprocessing.sequence.pad_sequences(vectorized_text, maxlen=None, dtype='int64', padding='post')\n",
    "length = Final_Text.shape[1]\n",
    "length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7GbViAAD79d"
   },
   "source": [
    "#### Importing the Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "ngAOaCRi5ksV",
    "outputId": "b734ef55-f1ae-48d4-d7f2-83f213c1b09f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.418</td>\n",
       "      <td>0.24968</td>\n",
       "      <td>-0.41242</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>0.34527</td>\n",
       "      <td>-0.044457</td>\n",
       "      <td>-0.49688</td>\n",
       "      <td>-0.17862</td>\n",
       "      <td>-0.00066023</td>\n",
       "      <td>-0.6566</td>\n",
       "      <td>0.27843</td>\n",
       "      <td>-0.14767</td>\n",
       "      <td>-0.55677</td>\n",
       "      <td>0.14658</td>\n",
       "      <td>-0.0095095</td>\n",
       "      <td>0.011658</td>\n",
       "      <td>0.10204</td>\n",
       "      <td>-0.12792</td>\n",
       "      <td>-0.8443</td>\n",
       "      <td>-0.12181</td>\n",
       "      <td>-0.016801</td>\n",
       "      <td>-0.33279</td>\n",
       "      <td>-0.1552</td>\n",
       "      <td>-0.23131</td>\n",
       "      <td>-0.19181</td>\n",
       "      <td>-1.8823</td>\n",
       "      <td>-0.76746</td>\n",
       "      <td>0.099051</td>\n",
       "      <td>-0.42125</td>\n",
       "      <td>-0.19526</td>\n",
       "      <td>4.0071</td>\n",
       "      <td>-0.18594</td>\n",
       "      <td>-0.52287</td>\n",
       "      <td>-0.31681</td>\n",
       "      <td>0.00059213</td>\n",
       "      <td>0.0074449</td>\n",
       "      <td>0.17778</td>\n",
       "      <td>-0.15897</td>\n",
       "      <td>0.012041</td>\n",
       "      <td>-0.054223</td>\n",
       "      <td>-0.29871</td>\n",
       "      <td>-0.15749</td>\n",
       "      <td>-0.34758</td>\n",
       "      <td>-0.045637</td>\n",
       "      <td>-0.44251</td>\n",
       "      <td>0.18785</td>\n",
       "      <td>0.0027849</td>\n",
       "      <td>-0.18411</td>\n",
       "      <td>-0.11514</td>\n",
       "      <td>-0.78581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.013441</td>\n",
       "      <td>0.23682</td>\n",
       "      <td>-0.16899</td>\n",
       "      <td>0.40951</td>\n",
       "      <td>0.63812</td>\n",
       "      <td>0.47709</td>\n",
       "      <td>-0.42852</td>\n",
       "      <td>-0.55641</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.23938</td>\n",
       "      <td>0.13001</td>\n",
       "      <td>-0.063734</td>\n",
       "      <td>-0.39575</td>\n",
       "      <td>-0.48162</td>\n",
       "      <td>0.23291</td>\n",
       "      <td>0.090201</td>\n",
       "      <td>-0.13324</td>\n",
       "      <td>0.078639</td>\n",
       "      <td>-0.41634</td>\n",
       "      <td>-0.15428</td>\n",
       "      <td>0.10068</td>\n",
       "      <td>0.48891</td>\n",
       "      <td>0.31226</td>\n",
       "      <td>-0.1252</td>\n",
       "      <td>-0.037512</td>\n",
       "      <td>-1.5179</td>\n",
       "      <td>0.12612</td>\n",
       "      <td>-0.02442</td>\n",
       "      <td>-0.042961</td>\n",
       "      <td>-0.28351</td>\n",
       "      <td>3.5416</td>\n",
       "      <td>-0.11956</td>\n",
       "      <td>-0.014533</td>\n",
       "      <td>-0.1499</td>\n",
       "      <td>0.21864</td>\n",
       "      <td>-0.33412</td>\n",
       "      <td>-0.13872</td>\n",
       "      <td>0.31806</td>\n",
       "      <td>0.70358</td>\n",
       "      <td>0.44858</td>\n",
       "      <td>-0.080262</td>\n",
       "      <td>0.63003</td>\n",
       "      <td>0.32111</td>\n",
       "      <td>-0.46765</td>\n",
       "      <td>0.22786</td>\n",
       "      <td>0.36034</td>\n",
       "      <td>-0.37818</td>\n",
       "      <td>-0.56657</td>\n",
       "      <td>0.044691</td>\n",
       "      <td>0.30392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.15164</td>\n",
       "      <td>0.30177</td>\n",
       "      <td>-0.16763</td>\n",
       "      <td>0.17684</td>\n",
       "      <td>0.31719</td>\n",
       "      <td>0.33973</td>\n",
       "      <td>-0.43478</td>\n",
       "      <td>-0.31086</td>\n",
       "      <td>-0.44999</td>\n",
       "      <td>-0.29486</td>\n",
       "      <td>0.16608</td>\n",
       "      <td>0.11963</td>\n",
       "      <td>-0.41328</td>\n",
       "      <td>-0.42353</td>\n",
       "      <td>0.59868</td>\n",
       "      <td>0.28825</td>\n",
       "      <td>-0.11547</td>\n",
       "      <td>-0.041848</td>\n",
       "      <td>-0.67989</td>\n",
       "      <td>-0.25063</td>\n",
       "      <td>0.18472</td>\n",
       "      <td>0.086876</td>\n",
       "      <td>0.46582</td>\n",
       "      <td>0.015035</td>\n",
       "      <td>0.043474</td>\n",
       "      <td>-1.4671</td>\n",
       "      <td>-0.30384</td>\n",
       "      <td>-0.023441</td>\n",
       "      <td>0.30589</td>\n",
       "      <td>-0.21785</td>\n",
       "      <td>3.746</td>\n",
       "      <td>0.0042284</td>\n",
       "      <td>-0.18436</td>\n",
       "      <td>-0.46209</td>\n",
       "      <td>0.098329</td>\n",
       "      <td>-0.11907</td>\n",
       "      <td>0.23919</td>\n",
       "      <td>0.1161</td>\n",
       "      <td>0.41705</td>\n",
       "      <td>0.056763</td>\n",
       "      <td>-6.3681e-05</td>\n",
       "      <td>0.068987</td>\n",
       "      <td>0.087939</td>\n",
       "      <td>-0.10285</td>\n",
       "      <td>-0.13931</td>\n",
       "      <td>0.22314</td>\n",
       "      <td>-0.080803</td>\n",
       "      <td>-0.35652</td>\n",
       "      <td>0.016413</td>\n",
       "      <td>0.10216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.70853</td>\n",
       "      <td>0.57088</td>\n",
       "      <td>-0.4716</td>\n",
       "      <td>0.18048</td>\n",
       "      <td>0.54449</td>\n",
       "      <td>0.72603</td>\n",
       "      <td>0.18157</td>\n",
       "      <td>-0.52393</td>\n",
       "      <td>0.10381</td>\n",
       "      <td>-0.17566</td>\n",
       "      <td>0.078852</td>\n",
       "      <td>-0.36216</td>\n",
       "      <td>-0.11829</td>\n",
       "      <td>-0.83336</td>\n",
       "      <td>0.11917</td>\n",
       "      <td>-0.16605</td>\n",
       "      <td>0.061555</td>\n",
       "      <td>-0.012719</td>\n",
       "      <td>-0.56623</td>\n",
       "      <td>0.013616</td>\n",
       "      <td>0.22851</td>\n",
       "      <td>-0.14396</td>\n",
       "      <td>-0.067549</td>\n",
       "      <td>-0.38157</td>\n",
       "      <td>-0.23698</td>\n",
       "      <td>-1.7037</td>\n",
       "      <td>-0.86692</td>\n",
       "      <td>-0.26704</td>\n",
       "      <td>-0.2589</td>\n",
       "      <td>0.1767</td>\n",
       "      <td>3.8676</td>\n",
       "      <td>-0.1613</td>\n",
       "      <td>-0.13273</td>\n",
       "      <td>-0.68881</td>\n",
       "      <td>0.18444</td>\n",
       "      <td>0.0052464</td>\n",
       "      <td>-0.33874</td>\n",
       "      <td>-0.078956</td>\n",
       "      <td>0.24185</td>\n",
       "      <td>0.36576</td>\n",
       "      <td>-0.34727</td>\n",
       "      <td>0.28483</td>\n",
       "      <td>0.075693</td>\n",
       "      <td>-0.062178</td>\n",
       "      <td>-0.38988</td>\n",
       "      <td>0.22902</td>\n",
       "      <td>-0.21617</td>\n",
       "      <td>-0.22562</td>\n",
       "      <td>-0.093918</td>\n",
       "      <td>-0.80375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.68047</td>\n",
       "      <td>-0.039263</td>\n",
       "      <td>0.30186</td>\n",
       "      <td>-0.17792</td>\n",
       "      <td>0.42962</td>\n",
       "      <td>0.032246</td>\n",
       "      <td>-0.41376</td>\n",
       "      <td>0.13228</td>\n",
       "      <td>-0.29847</td>\n",
       "      <td>-0.085253</td>\n",
       "      <td>0.17118</td>\n",
       "      <td>0.22419</td>\n",
       "      <td>-0.10046</td>\n",
       "      <td>-0.43653</td>\n",
       "      <td>0.33418</td>\n",
       "      <td>0.67846</td>\n",
       "      <td>0.057204</td>\n",
       "      <td>-0.34448</td>\n",
       "      <td>-0.42785</td>\n",
       "      <td>-0.43275</td>\n",
       "      <td>0.55963</td>\n",
       "      <td>0.10032</td>\n",
       "      <td>0.18677</td>\n",
       "      <td>-0.26854</td>\n",
       "      <td>0.037334</td>\n",
       "      <td>-2.0932</td>\n",
       "      <td>0.22171</td>\n",
       "      <td>-0.39868</td>\n",
       "      <td>0.20912</td>\n",
       "      <td>-0.55725</td>\n",
       "      <td>3.8826</td>\n",
       "      <td>0.47466</td>\n",
       "      <td>-0.95658</td>\n",
       "      <td>-0.37788</td>\n",
       "      <td>0.20869</td>\n",
       "      <td>-0.32752</td>\n",
       "      <td>0.12751</td>\n",
       "      <td>0.088359</td>\n",
       "      <td>0.16351</td>\n",
       "      <td>-0.21634</td>\n",
       "      <td>-0.094375</td>\n",
       "      <td>0.018324</td>\n",
       "      <td>0.21048</td>\n",
       "      <td>-0.03088</td>\n",
       "      <td>-0.19722</td>\n",
       "      <td>0.082279</td>\n",
       "      <td>-0.09434</td>\n",
       "      <td>-0.073297</td>\n",
       "      <td>-0.064699</td>\n",
       "      <td>-0.26044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1         2   ...         47         48        49\n",
       "the     0.418    0.24968  -0.41242  ...   -0.18411   -0.11514  -0.78581\n",
       ",    0.013441    0.23682  -0.16899  ...   -0.56657   0.044691   0.30392\n",
       ".     0.15164    0.30177  -0.16763  ...   -0.35652   0.016413   0.10216\n",
       "of    0.70853    0.57088   -0.4716  ...   -0.22562  -0.093918  -0.80375\n",
       "to    0.68047  -0.039263   0.30186  ...  -0.073297  -0.064699  -0.26044\n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" The glove.6B.50d.txt was taken and a dataframe with the embedding for all the words within glove.6B.50d.txt was created.\n",
    "    input: \n",
    "        text file:glove.6B.50d.txt\n",
    "    return:\n",
    "        dataframe: glove_df  \"\"\"\n",
    "glove = []\n",
    "\n",
    "with open(\"glove.6B.50d.txt\") as file:\n",
    "    i = 0\n",
    "    for line in file:\n",
    "        glove.append(line.rstrip())\n",
    "        i += 1\n",
    "\n",
    "glove_dict ={}\n",
    "\n",
    "for word in glove:\n",
    "    vec = word.split()\n",
    "    glove_dict[vec[0]] = vec[1:]\n",
    "\n",
    "glove_df = pd.DataFrame(data=glove_dict).transpose() \n",
    "\n",
    "glove_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hf5Gykx7D1ZY"
   },
   "source": [
    "#### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "XsoN9QQC6CPU"
   },
   "outputs": [],
   "source": [
    "\"\"\" The followinf functions were created:\n",
    "    recall_m: This function is made to calculate the recall score and is used to calculate the f1 score\n",
    "    precision_m: This function is made to calculate the precision score and is used to calculate the f1 score  \n",
    "    f1: This function is made to calculate the precision score and is used to calculate the effecticveness of the model\"\"\"\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pxpgxme8z5mG",
    "outputId": "3118fc2e-7b7a-44f9-c968-f96d0d7c9068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "191/191 [==============================] - 8s 34ms/step - loss: 0.5827 - acc: 0.6949 - f1: 0.4415\n",
      "Epoch 2/10\n",
      "191/191 [==============================] - 7s 35ms/step - loss: 0.3067 - acc: 0.8775 - f1: 0.8443\n",
      "Epoch 3/10\n",
      "191/191 [==============================] - 7s 34ms/step - loss: 0.2122 - acc: 0.9430 - f1: 0.9277\n",
      "Epoch 4/10\n",
      "191/191 [==============================] - 7s 34ms/step - loss: 0.0895 - acc: 0.9673 - f1: 0.9601\n",
      "Epoch 5/10\n",
      "191/191 [==============================] - 7s 34ms/step - loss: 0.0491 - acc: 0.9831 - f1: 0.9797\n",
      "Epoch 6/10\n",
      "191/191 [==============================] - 7s 34ms/step - loss: 0.0597 - acc: 0.9839 - f1: 0.9797\n",
      "Epoch 7/10\n",
      "191/191 [==============================] - 7s 34ms/step - loss: 0.0254 - acc: 0.9926 - f1: 0.9910\n",
      "Epoch 8/10\n",
      "191/191 [==============================] - 7s 34ms/step - loss: 0.0164 - acc: 0.9943 - f1: 0.9930\n",
      "Epoch 9/10\n",
      "191/191 [==============================] - 7s 35ms/step - loss: 0.0129 - acc: 0.9947 - f1: 0.9935\n",
      "Epoch 10/10\n",
      "191/191 [==============================] - 7s 34ms/step - loss: 0.0124 - acc: 0.9954 - f1: 0.9943\n",
      "48/48 [==============================] - 1s 7ms/step - loss: 4839.9766 - acc: 0.7479 - f1: 0.7126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4839.9765625, 0.7478660345077515, 0.7126083970069885]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Creating a model using the rnn network.\n",
    "    input: \n",
    "        Final_Text and data['target']\n",
    "    return:\n",
    "        f1 score \"\"\"\n",
    "x_train, x_val, y_train, y_val = train_test_split(Final_Text, data['target'],test_size= 0.2,random_state=411)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim =  20000, output_dim = 50, input_length=length))\n",
    "model.add(LSTM(64,activation='relu',return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc',f1])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10)\n",
    "\n",
    "model.evaluate(x_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq-pHMcnOv_E"
   },
   "source": [
    "#### Predicting testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ObtFC1n_Ozks",
    "outputId": "7c87bee7-cb7c-42b7-998a-9957188f892d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the dataset and looking at the top 5 values of the dataset\n",
    "data = pd.read_csv('test.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "yaeRKSf_PQEZ"
   },
   "outputs": [],
   "source": [
    "\"\"\" Cleaning th data by dropping the id, keyword, location from the dataframe\n",
    "    input: \n",
    "        The entire training dataset\n",
    "    return:\n",
    "        Dataset with only the text and target files \"\"\"\n",
    "data = data.drop(['id','keyword','location'],axis=1)\n",
    "data1 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "0BZVD-kFPq_c"
   },
   "outputs": [],
   "source": [
    "\"\"\" Creating the vectorize class with a standardize finction. The standerdize function helps remove all the puctuations\n",
    "    as well as change all the letter to lower letters. This is needed to standardize the text and only keep the text.\n",
    "    input: \n",
    "        string: All residents asked to 'shelter in place' are\n",
    "    return:\n",
    "        string: all residents asked to shelter in place are \"\"\"\n",
    "class Vectorizer:\n",
    "    def standardize(self, input_text):\n",
    "        text = input_text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "v1 = Vectorizer()\n",
    "data1['text'] = data1['text'].apply(lambda x : v1.standardize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "4amqO6B3PeWM",
    "outputId": "f2ed4763-c630-42d7-90db-e6bc78755612"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heard about earthquake is different cities sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond geese are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>typhoon soudelor kills 28 in china and taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                 just happened a terrible car crash\n",
       "1  heard about earthquake is different cities sta...\n",
       "2  there is a forest fire at spot pond geese are ...\n",
       "3              apocalypse lighting spokane wildfires\n",
       "4      typhoon soudelor kills 28 in china and taiwan"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the top 5 rows of the clean data\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "eBcMwQWyPl-o"
   },
   "outputs": [],
   "source": [
    "\"\"\" converting the dataset into a dataframe so that the vectorization can be performed on the text data.\n",
    "    input: \n",
    "        dataframe:data['text']\n",
    "    return:\n",
    "        dataset: text \"\"\"\n",
    "text = tf.data.Dataset.from_tensor_slices((data1['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "_VKndYuJPvYW"
   },
   "outputs": [],
   "source": [
    "\"\"\" The following steps are don in this code chunk:\n",
    "    vectorize_layer was used on the data['text'] to convert the text to numeric data.\n",
    "    Padding the text so that none of the information is lost while running it in the model.\n",
    "    identifying the \n",
    "    input: \n",
    "        string: i am good\n",
    "    return:\n",
    "        numpy array:[15,27,1302]  \"\"\"\n",
    "vectorized_text = vectorize_layer(data1['text'])\n",
    "Final_Text_padd = tf.keras.preprocessing.sequence.pad_sequences(vectorized_text, maxlen=None, dtype='int64', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "BZMCLhS4PyKe"
   },
   "outputs": [],
   "source": [
    "\"\"\" Predicting the target variable\n",
    "    input: \n",
    "        the text of the testing data\n",
    "    return:\n",
    "        Prediction  \"\"\"\n",
    "pred = model.predict(Final_Text_padd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "dAqSHOnyQdQj"
   },
   "outputs": [],
   "source": [
    "\"\"\" creating a csv with the text as well as the prediction values between 0 or 1\n",
    "    input: \n",
    "        text data as well as the pred data\n",
    "    return:\n",
    "        Group10.csv\"\"\"\n",
    "pred_sigmoid = np.where(pred > 0.5, 1, 0)\n",
    "text_col = np.asarray(data['text'])\n",
    "text_col = np.expand_dims(text_col, axis=1)\n",
    "csv_pred = np.concatenate((text_col, pred_sigmoid), axis = 1)\n",
    "pd.DataFrame(csv_pred).to_csv(\"Group10.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "UbJLJSoJVsKO"
   },
   "outputs": [],
   "source": [
    "model.save('Group_10_Lab_6.keras', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oi-QI99GVsZ4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Group_10_LAB_6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
